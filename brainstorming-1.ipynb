{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the texts\n",
    "def encode_data(tokenizer, texts, max_length=512):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess_data(dataset, split):\n",
    "    texts = [example['text'] for example in dataset[split]]\n",
    "    labels = [example['label'] for example in dataset[split]]\n",
    "    encodings = encode_data(tokenizer, texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
    "    return dataset.shuffle(1000).batch(8)\n",
    "\n",
    "train_dataset = preprocess_data(dataset, 'train')\n",
    "val_dataset = preprocess_data(dataset, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "input_ids = Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT embeddings\n",
    "bert_outputs = bert_model(inputs)[0]\n",
    "x = GlobalAveragePooling1D()(bert_outputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(val_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
